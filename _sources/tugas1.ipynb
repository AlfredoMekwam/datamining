{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN1dIJqd5Iph9EWyeRFN0RR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Tugas 1"],"metadata":{"id":"RmjIIF-XYukC"}},{"cell_type":"code","source":["Walmart - Perkiraan Penjualan Toko"],"metadata":{"id":"9VsHuWoN1NKx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. DATA LOADING\n","In this challenge, the following files were available:\n","\n","stores.csv\n","features.csv\n","train.csv\n","test.csv\n","sampleSubmission.csv\n","The \"store\" and \"features\" files have been combined and joined to the \"train\" and \"test\" datasets. The sampleSubmission file is the template to be used for submission to Kaggle and will be used at the end. The submission dataset is equivalent to the test dataset but only with the triple (store, department and date) to be predicted."],"metadata":{"id":"7IptIxGV1OZA"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","warnings.filterwarnings('ignore')\n","df_features = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/features.csv.zip', sep=',')\n","df_stores = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/stores.csv', sep=',')\n","\n","df_features_stores = df_features.merge(df_stores, how='inner', on='Store')\n","df_features_stores.head()\n","df_train = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/train.csv.zip', sep=',')\n","train = df_train.merge(df_features_stores, how='inner', on=['Store','Date','IsHoliday'])\n","train.head()\n","df_test = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/test.csv.zip', sep=',')\n","test = df_test.merge(df_features_stores, how='inner', on=['Store','Date','IsHoliday'])\n","test.head()"],"metadata":{"id":"ETP1awxs1fbR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Checking the first and last records of the training and test datasets. The train dataset contains 421570 weekly sales records detailed by stores and departments from 02-05-2010 to 10-26-2012. The test base starts one week later and runs until 07-26-2013."],"metadata":{"id":"cEH4Kk_e2GMA"}},{"cell_type":"code","source":["print(\"Primeiro registro treino: \", train['Date'].min())\n","print(\"Último registro treino:\", train['Date'].max())\n","\n","print(\"Primeiro registro teste: \", test['Date'].min())\n","print(\"Último registro teste:\", test['Date'].max())"],"metadata":{"id":"ETvTnC3K2M18"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Since the records are weekly, the \"date\" variable was converted to week of the year and year, as two new variables."],"metadata":{"id":"y_zILeQE2bw9"}},{"cell_type":"code","source":["train['Date'] = pd.to_datetime(train['Date'])\n","test['Date'] = pd.to_datetime(test['Date'])\n","\n","train['Week'] = train['Date'].dt.isocalendar().week\n","test['Week'] = test['Date'].dt.isocalendar().week\n","\n","train['Year'] = train['Date'].dt.isocalendar().year\n","test['Year'] = test['Date'].dt.isocalendar().year"],"metadata":{"id":"k-J8xQWc2gQh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["By analyzing a summary of the training and test dataset, there is a certain similarity in the patterns of both datasets, without much discrepancy in the values."],"metadata":{"id":"UAHp4B-j2rrl"}},{"cell_type":"code","source":["train.describe()"],"metadata":{"id":"tAJjALCo2yFA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test.describe()"],"metadata":{"id":"VY8qZaAT26qj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. FEATURES TYPES"],"metadata":{"id":"ijyUDefD3oZ_"}},{"cell_type":"markdown","source":["2.1 TRANSFORMATION"],"metadata":{"id":"YEquEEcM3usk"}},{"cell_type":"markdown","source":["Analyzing the type of variables, it is observed that all of them are in numeric format, except for the Date, Type and IsHoliday variables. The \"Date\" variable will not be used for training the model, using only Week and Year. \"IsHoliday\" was transformed to numeric binary and \"Type\" to ordinal numeric format. These transformation was applied to train and test datasets."],"metadata":{"id":"x9pgntBv3y-Q"}},{"cell_type":"code","source":["train.dtypes"],"metadata":{"id":"4ehormGl37m3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train['Type'].unique()"],"metadata":{"id":"GfvcVJaW4ID0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train['Date'] = pd.to_datetime(train['Date'])\n","train['Type'] = train['Type'] .apply(lambda x: 3 if x == 'A' else(2 if x == 'B' else 1))\n","train['IsHoliday'] = train['IsHoliday'].apply(lambda x: 1 if x == True else 0)\n","\n","cols = train.columns.drop(['Date'])\n","train[cols] = train[cols].apply(pd.to_numeric, errors='coerce')"],"metadata":{"id":"73jXqIY64JXn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test['Date'] = pd.to_datetime(test['Date'])\n","test['Type'] = test['Type'].apply(lambda x: 3 if x == 'A' else(2 if x == 'B' else 1))\n","test['IsHoliday'] = test['IsHoliday'].apply(lambda x: 1 if x == True else 0)\n","\n","cols = test.columns.drop(['Date'])\n","test[cols] = test[cols].apply(pd.to_numeric, errors='coerce')"],"metadata":{"id":"XZlro1Y24O-G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2.2. HOLIDAYS"],"metadata":{"id":"zdNl6wAg4UDC"}},{"cell_type":"markdown","source":["According to the challenge instructions, the holiday dates are expected to have a greater weight in the model training, since in general they represent a greater volume of sales.\n","\n","The code below shows all dates that represent holidays, both in the train and test dataset. It is observed that the holidays are in the same weeks (6, 36, 47 and 52) for the years 2010, 2011, 2012 and 2013. From the data provided by the challenge, it is possible to identify what these holidays are.\n","\n","Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13 --> WEEK 6\n","Labor Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13 --> WEEK 36\n","Thanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13 --> WEEK 47\n","Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13 --> WEEK 52\n","It is noticed that there are no sales records on the laborday holiday at the test dataset, since the holiday is in September and the test runs until July.\n","\n"],"metadata":{"id":"fnxwjRn04do5"}},{"cell_type":"code","source":["holiday_train = train[['Date','Week','Year','IsHoliday']]\n","holiday_train = holiday_train.loc[holiday_train['IsHoliday']==True].drop_duplicates()\n","\n","holiday_test = test[['Date','Week','Year','IsHoliday']]\n","holiday_test = holiday_test.loc[holiday_test['IsHoliday']==True].drop_duplicates()\n","\n","holidays = pd.concat([holiday_train, holiday_test])\n","holidays"],"metadata":{"id":"46dKs7GU5iOx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In order to identify not only if it is holiday, but also which holiday it is, and try to improve the sales volume prediction for these dates, the IsHoliday binary variable was transformed to:"],"metadata":{"id":"kJKbORLB5ury"}},{"cell_type":"markdown","source":["0 - if it is not a holiday\n","1 - if the holiday is SuperBowl\n","2 - if the holiday is LaborDay\n","3 - if the holiday is Thanksgiving\n","4 - if the holiday is Christmas"],"metadata":{"id":"nR0gmThH5734"}},{"cell_type":"code","source":["def holiday_type(x):\n","    if   (x['IsHoliday']== 1) & (x['Week']==6):\n","       return 1 #SuperBowl\n","    elif (x['IsHoliday']== 1) & (x['Week']==36):\n","       return 2 #LaborDay\n","    elif (x['IsHoliday']== 1) & (x['Week']==47):\n","       return 3 #Thanksgiving\n","    elif (x['IsHoliday']== 1) & (x['Week']==52):\n","       return 4 #Christmas\n","    else:\n","       return 0"],"metadata":{"id":"GJm-yQxG6Dfo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train['IsHoliday'] = train.apply(holiday_type, axis=1)\n","train['IsHoliday'].unique()"],"metadata":{"id":"8SuS0GKJ6L3C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test['IsHoliday'] = test.apply(holiday_type, axis=1)\n","test['IsHoliday'].unique()"],"metadata":{"id":"bpfD9QV96Rc-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. EXPLORATORY ANALYSIS"],"metadata":{"id":"joU-Gkjx6c1r"}},{"cell_type":"markdown","source":["NULLS AND CORRELATIONS"],"metadata":{"id":"HEzuZGAz6d3f"}},{"cell_type":"markdown","source":["The training dataset has a percentage of 64% to 74% of null records for the MarkDown variables. However, before removing them, we will analyze their correlation with the other variables in order to check their impact on weekly sales."],"metadata":{"id":"rm_PZuxS6ibx"}},{"cell_type":"code","source":["train = train.replace('None', np.nan)\n","train = train.replace('NaN', np.nan)\n","train = train.replace('NaT', np.nan)\n","train = train.replace('', np.nan)\n","train_nulls = (train.isnull().sum(axis = 0)/len(train))*100\n","train_nulls"],"metadata":{"id":"377oL_-f6n2o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Analyzing the correlation matrix of the training database, it can be noticed a weak correlation between tWeekly_Sales and the other variables. The variables \"size\", followed by the \"type\" and \"dept\" variables, appear to have the greatest impact on Weekly Sales."],"metadata":{"id":"1egJjNFb6xdP"}},{"cell_type":"code","source":["plt.figure(figsize=(15, 10))\n","\n","heatmap = sns.heatmap(train.corr(), vmin=-1, vmax=1, annot=True,cmap=\"Blues\",annot_kws={\"fontsize\":10})\n","heatmap.set_title('Correlation Matrix - Train', fontdict={'fontsize':12}, pad=12);"],"metadata":{"id":"PlSpP7aX61TV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Unlike the train dataset, the test has a much lower percentage of null in the MarkDown variables, and about 33% of null records in the CPI and Unemployment variables.\n","\n","The correlation matrix shows that the \"Markdown\" variables have a certain correlation with the IsHoliday variable, which can help in predicting. Therefore, these variables will not be eliminated at first.\n","\n","Null records have been replaced by zero in both datasets."],"metadata":{"id":"9bNwZNdh7B2h"}},{"cell_type":"code","source":["test = test.replace('None', np.nan)\n","test = test.replace('NaN', np.nan)\n","test = test.replace('NaT', np.nan)\n","test = test.replace('', np.nan)\n","test_nulls = (test.isnull().sum(axis = 0)/len(test))*100\n","test_nulls"],"metadata":{"id":"uAOTCbzP7C2W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(15, 10))\n","\n","heatmap = sns.heatmap(test.corr(), vmin=-1, vmax=1, annot=True,cmap=\"Blues\",annot_kws={\"fontsize\":10})\n","heatmap.set_title('Correlation Matrix - Test', fontdict={'fontsize':12}, pad=12);"],"metadata":{"id":"ZNVi2Ecw7MZu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = train.fillna(0)\n","test = test.fillna(0)\n","\n","train.isnull().sum()"],"metadata":{"id":"800li_A07Vju"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["AVG OF SALES X WEEK X YEAR"],"metadata":{"id":"oP1OQ-fJ7bZQ"}},{"cell_type":"markdown","source":["Weekly sales data were grouped by week and year in order to identify the average and median sales per week over the years.\n","\n","In general, the average values are well above the median, which indicates a high dispersion and variation in sales by stores and departments in a week.\n","\n","Despite this, there is a certain pattern over the years, with high seasonality at the end of the year."],"metadata":{"id":"HdBoW94H7cVm"}},{"cell_type":"code","source":["weekly_sales = train.groupby(['Year','Week']).agg({'Weekly_Sales': ['mean', 'median']})\n","weekly_sales2010 = train.loc[train['Year']==2010].groupby(['Week']).agg({'Weekly_Sales': ['mean', 'median']})\n","weekly_sales2011 = train.loc[train['Year']==2011].groupby(['Week']).agg({'Weekly_Sales': ['mean', 'median']})\n","weekly_sales2012 = train.loc[train['Year']==2012].groupby(['Week']).agg({'Weekly_Sales': ['mean', 'median']})"],"metadata":{"id":"DalvdJfm7hu-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["weekly_sales.plot(figsize=(20,5))"],"metadata":{"id":"hvHqT4Ll7nIG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The data was also grouped by week but separately for each year, in order to identify patterns between the weeks of different years. As a result, a similar pattern can be seen over the years, with a significant increase in sales in weeks 51 and 47 (Christmas and Thanksgiving). The Superbowl (week 6) and LaborDay holidays (week 36) have little impact on increased sales volume."],"metadata":{"id":"-Ec7VVBz70K4"}},{"cell_type":"code","source":["plt.figure(figsize=(20, 7))\n","\n","sns.lineplot(weekly_sales2010['Weekly_Sales']['mean'].index, weekly_sales2010['Weekly_Sales']['mean'].values)\n","sns.lineplot(weekly_sales2011['Weekly_Sales']['mean'].index, weekly_sales2011['Weekly_Sales']['mean'].values)\n","sns.lineplot(weekly_sales2012['Weekly_Sales']['mean'].index, weekly_sales2012['Weekly_Sales']['mean'].values)\n","\n","plt.grid()\n","plt.xticks(np.arange(1, 53, step=1))\n","plt.legend(['2010', '2011', '2012'])\n","plt.show()"],"metadata":{"id":"Zv2D0GFq74HI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["STORES X WEEK SALES"],"metadata":{"id":"zA3gMY_W8E2d"}},{"cell_type":"markdown","source":["Analyzing the average weekly sales per store, there is a strong variation in sales volume between stores, ranging from 5000 up to 30000."],"metadata":{"id":"YSz-Y0DG8JAO"}},{"cell_type":"code","source":["stores = train.groupby(['Store']).agg({'Weekly_Sales': ['mean']})\n","\n","plt.figure(figsize=(20, 7))\n","plt.bar(stores.index,stores['Weekly_Sales']['mean'])\n","plt.xticks(np.arange(1, 46, step=1))\n","plt.ylabel('Week Sales', fontsize=16)\n","plt.xlabel('Store', fontsize=16)\n","plt.show()"],"metadata":{"id":"BbmMDDeX8KGA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Despite this discrepancy in weekly sales by store, this behavior seems to remain stable over the years. Some stores showed a decrease in sales over the years, such as stores 14, 27, 35 and 36."],"metadata":{"id":"kBYRsH_G8TgF"}},{"cell_type":"code","source":["stores_sales2010 = train.loc[train['Year']==2010].groupby(['Store']).agg({'Weekly_Sales': ['mean', 'median']})\n","stores_sales2011 = train.loc[train['Year']==2011].groupby(['Store']).agg({'Weekly_Sales': ['mean', 'median']})\n","stores_sales2012 = train.loc[train['Year']==2012].groupby(['Store']).agg({'Weekly_Sales': ['mean', 'median']})\n","\n","plt.figure(figsize=(20, 7))\n","sns.lineplot(stores_sales2010['Weekly_Sales']['mean'].index, stores_sales2010['Weekly_Sales']['mean'].values)\n","sns.lineplot(stores_sales2011['Weekly_Sales']['mean'].index, stores_sales2011['Weekly_Sales']['mean'].values)\n","sns.lineplot(stores_sales2012['Weekly_Sales']['mean'].index, stores_sales2012['Weekly_Sales']['mean'].values)\n","\n","plt.xticks(np.arange(1, 46, step=1))\n","plt.legend(['2010', '2011', '2012'])\n","plt.ylabel('Week Sales', fontsize=16)\n","plt.xlabel('Store', fontsize=16)\n","plt.show()"],"metadata":{"id":"x720GzwT8URc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DEPARTAMENT x WEEK SALES"],"metadata":{"id":"5F8_KfTB8e7z"}},{"cell_type":"markdown","source":["Weekly sales by department are even more irregular, with departments with average sales ranging from 0 to more than 70000."],"metadata":{"id":"70JtdNQY8grt"}},{"cell_type":"code","source":["departament = train.groupby(['Dept']).agg({'Weekly_Sales': ['mean', 'median']})\n","\n","plt.figure(figsize=(20, 7))\n","plt.bar(departament.index,departament['Weekly_Sales']['mean'])\n","plt.xticks(np.arange(1, 100, step=2))\n","plt.ylabel('Week Sales', fontsize=16)\n","plt.xlabel('Departament', fontsize=16)\n","plt.show()"],"metadata":{"id":"j6eedjv78nf_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Despite this discrepancy in weekly sales by departament, this behavior seems to remain stable over the years. Some departaments showed a decrease in sales over the years, such as departaments 18, 65 and 73."],"metadata":{"id":"H_3b1xOZ82Uh"}},{"cell_type":"code","source":["departament_sales2010 = train.loc[train['Year']==2010].groupby(['Dept']).agg({'Weekly_Sales': ['mean', 'median']})\n","departament_sales2011 = train.loc[train['Year']==2011].groupby(['Dept']).agg({'Weekly_Sales': ['mean', 'median']})\n","departament_sales2012 = train.loc[train['Year']==2012].groupby(['Dept']).agg({'Weekly_Sales': ['mean', 'median']})\n","\n","plt.figure(figsize=(20, 7))\n","sns.lineplot(departament_sales2010['Weekly_Sales']['mean'].index, departament_sales2010['Weekly_Sales']['mean'].values)\n","sns.lineplot(departament_sales2011['Weekly_Sales']['mean'].index, departament_sales2011['Weekly_Sales']['mean'].values)\n","sns.lineplot(departament_sales2012['Weekly_Sales']['mean'].index, departament_sales2012['Weekly_Sales']['mean'].values)\n","\n","plt.xticks(np.arange(1, 100, step=2))\n","plt.legend(['2010', '2011', '2012'])\n","\n","plt.ylabel('Week Sales', fontsize=16)\n","plt.xlabel('Departament', fontsize=16)\n","plt.show()"],"metadata":{"id":"-GT9tp2H87rz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["SIZE x WEEK SALES"],"metadata":{"id":"IoWKsO7a9BT-"}},{"cell_type":"markdown","source":["Grouping weekly sales by store size, the chart below seems to indicate a certain trend towards higher sales for larger stores.\n","\n","However, this relationship is far from being proportionally linear, with several cases contradicting this trend."],"metadata":{"id":"WcF9bgVW9BzQ"}},{"cell_type":"code","source":["size = train.groupby(['Size']).agg({'Weekly_Sales': ['mean']})\n","\n","plt.figure(figsize=(20, 7))\n","plt.plot(size)\n","#plt.xticks(np.arange(1, 100, step=2))\n","#plt.show()\n","\n","plt.ylabel('Week Sales', fontsize=16)\n","plt.xlabel('Size', fontsize=16)"],"metadata":{"id":"hjZkEZ7m9Fh2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The pattern of weekly sales by store size seems stable over the years, despite some cases of increase or decrease in sales of stores of the same size from 2010 to 2012."],"metadata":{"id":"4aUj5fuc9OzF"}},{"cell_type":"code","source":["size_sales2010 = train.loc[train['Year']==2010].groupby(['Size']).agg({'Weekly_Sales': ['mean', 'median']})\n","size_sales2011 = train.loc[train['Year']==2011].groupby(['Size']).agg({'Weekly_Sales': ['mean', 'median']})\n","size_sales2012 = train.loc[train['Year']==2012].groupby(['Size']).agg({'Weekly_Sales': ['mean', 'median']})\n","\n","plt.figure(figsize=(20, 7))\n","sns.lineplot(size_sales2010['Weekly_Sales']['mean'].index, size_sales2010['Weekly_Sales']['mean'].values)\n","sns.lineplot(size_sales2011['Weekly_Sales']['mean'].index, size_sales2011['Weekly_Sales']['mean'].values)\n","sns.lineplot(size_sales2012['Weekly_Sales']['mean'].index, size_sales2012['Weekly_Sales']['mean'].values)\n","\n","plt.legend(['2010', '2011', '2012'])\n","plt.ylabel('Week Sales', fontsize=16)\n","plt.xlabel('Size', fontsize=16)\n","plt.show()"],"metadata":{"id":"egGDtyDT9Pnt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["TYPE x WEEK SALES"],"metadata":{"id":"QDxYrMX89XoE"}},{"cell_type":"markdown","source":["The 'type' feature also seems to have a certain relationship with Weekly Sales. Type 'A' (transformed to '3') has a higher sales median than types 'B' and 'C', in addition to a greater dispersion of sales values around this median. Type 'C' (transformed to '1') tends to have lower weekly sales."],"metadata":{"id":"NH9XEILB9YYQ"}},{"cell_type":"code","source":["plt.figure(figsize=(10, 7))\n","sns.boxplot(x='Type', y='Weekly_Sales', data=train,showfliers = False)"],"metadata":{"id":"baiLDNGs9cqf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Despite this differentiation around median, the three types have many outlier records."],"metadata":{"id":"IkVLGpnm9nvD"}},{"cell_type":"code","source":["plt.figure(figsize=(15, 7))\n","sns.boxplot(x='Type', y='Weekly_Sales', data=train,showfliers = True)"],"metadata":{"id":"hBdyz7N29o86"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4. EVALUATION FUNCTION"],"metadata":{"id":"CvGx60mH9w0S"}},{"cell_type":"markdown","source":["The challenge evaluation is based on Weighted Mean Absolute Error (WMAE), with a weight of 5 for Holiday Weeks and 1 otherwise. A function was created to evaluate the model considering these criteria."],"metadata":{"id":"J4tLi-Aj9x8I"}},{"cell_type":"code","source":["sample_weight = train['IsHoliday'].apply(lambda x: 1 if x==0 else 5)\n","sample_weight_frame = pd.DataFrame(sample_weight, index=train.index)"],"metadata":{"id":"IylJCR1K91SA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import make_scorer\n","\n","def WMAE(y_test, y_pred):\n","        y_pred_df = pd.DataFrame(y_pred,index=y_test.index)\n","        \n","        weights_5 = sample_weight_frame.loc[(y_test.index)].loc[sample_weight_frame.IsHoliday==5].index\n","        weights_1 = sample_weight_frame.loc[(y_test.index)].loc[sample_weight_frame.IsHoliday==1].index\n","        \n","        sum_5 = np.sum(5*(abs(y_test.loc[weights_5].values-y_pred_df.loc[weights_5].values)))\n","        sum_1 = np.sum(abs(y_test.loc[weights_1].values-y_pred_df.loc[weights_1].values))           \n","        \n","        return np.round((sum_5+sum_1)/(5*len(weights_5)+len(weights_1)),2)\n"," \n","my_score = make_scorer(WMAE,greater_is_better=False)"],"metadata":{"id":"Ve3ehARB-xAI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5. MODEL TRAINING"],"metadata":{"id":"Ds3_CwTP-5pX"}},{"cell_type":"markdown","source":["5.1. TRAINING WITH ALL FEATURES"],"metadata":{"id":"HvMwsexI-6lQ"}},{"cell_type":"markdown","source":["The training models were initially fitted with all the features of the train dataset. In order to select the best regression algorithm for this model, Random Search was applied to some of the main regression algorithms.The RandomForestRegressor algorithm obtained the best result."],"metadata":{"id":"giYLlFgl-95F"}},{"cell_type":"code","source":["train_all = train.drop(['Date'],axis=1)\n","train_all"],"metadata":{"id":"yuuU9EBd_Lb-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train_all = train_all.loc[:, ['Weekly_Sales']]\n","x_train_all = train_all.drop(['Weekly_Sales'], axis=1)"],"metadata":{"id":"NOt98M08_UQv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","x_train, x_test, y_train, y_test = train_test_split(x_train_all, y_train_all, test_size=0.2, random_state=0)\n","\n","print(x_train.shape)\n","print(x_test.shape)\n","(337256, 16)\n","(84314, 16)"],"metadata":{"id":"09-ZiZOn_U7g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#RandomForest, ExtraTrees, XGB\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","from sklearn.pipeline import Pipeline\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import make_scorer\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.neighbors import KNeighborsRegressor\n","from xgboost import XGBRegressor\n","\n","clf = RandomForestRegressor(random_state=0)\n","pca = PCA()\n","\n","pipe = Pipeline(steps=[('clf', clf)])\n","\n","param_grid = [ {\n","                'clf':[RandomForestRegressor()],\n","                'clf__n_estimators': [50,100,150],\n","                'clf__max_depth': [10,20,30]\n","                },\n","               \n","                {\n","                'clf': [ExtraTreesRegressor()],\n","                'clf__n_estimators': [50,100,150],\n","                'clf__max_depth': [10,20,30]\n","                },\n","               \n","                {\n","                'clf': [XGBRegressor()],  \n","                'clf__learning_rate':[0.1,0.05],\n","                'clf__min_samples_split':[5,7,9],\n","                'clf__max_depth':[10,20,30]\n","                }\n","              ]\n","\n","rscv_all_tree = RandomizedSearchCV(pipe, param_grid, cv = 3, scoring = my_score, n_jobs=-1)\n","model_all_tree = rscv_all_tree.fit(x_train, y_train)"],"metadata":{"id":"vROzWLLw_kYD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rscv_all_tree.best_estimator_"],"metadata":{"id":"izaQ40_V_0y8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred = rscv_all_tree.best_estimator_.predict(x_test)\n","print('WMAE:', WMAE(y_test, y_pred))"],"metadata":{"id":"Kb4Wnzwj_6nQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5.2. TRAINING WITH MAIN FEATURES"],"metadata":{"id":"KQn2iyhqACCm"}},{"cell_type":"markdown","source":["In an attempt to obtain even better results in the prediction, models were also trained only with the features of greatest impact in the \"Weekly Sales\", based on the correlation matrix.\n","\n","Therefore, the features with the highest correlation (\"Size\", \"Type\" and \"Dept\") were used to train these models, in addition to \"IsHoliday\", needed to calculate the evaluation metric and the features \"Store\", \"Week\" and \"Year\", essential for identifying the record and future prediction.\n","\n","The results show that the models using only the most relevant features performs better than the models using all variables."],"metadata":{"id":"n6hBX1jXAC9d"}},{"cell_type":"code","source":["plt.figure(figsize=(15, 10))\n","\n","heatmap = sns.heatmap(train.corr(), vmin=-1, vmax=1, annot=True,cmap=\"Blues\",annot_kws={\"fontsize\":10})\n","heatmap.set_title('Matriz de Correlação', fontdict={'fontsize':12}, pad=12);"],"metadata":{"id":"-P8XtLnqALoK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_relevant = train.drop(['Date','Temperature','Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5','CPI','Unemployment'],axis=1)\n","train_relevant"],"metadata":{"id":"CYjuP2kMAVFu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_relevant = train_relevant.loc[:, ['Weekly_Sales']]\n","x_relevant = train_relevant.drop(['Weekly_Sales'], axis=1)"],"metadata":{"id":"5DOd0KfzAZrY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","x_train_relevant, x_test_relevant, y_train_relevant, y_test_relevant = train_test_split(x_relevant, y_relevant, test_size=0.2, random_state=0)\n","\n","print(x_train_relevant.shape)\n","print(x_test_relevant.shape)"],"metadata":{"id":"VUZHurhzAajF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#RandomForest, ExtraTrees, XGB\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","from sklearn.pipeline import Pipeline\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import make_scorer\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.neighbors import KNeighborsRegressor\n","from xgboost import XGBRegressor\n","import numpy as np\n","\n","clf = RandomForestRegressor(random_state=0)\n","pca = PCA()\n","\n","pipe = Pipeline(steps=[('clf', clf)])\n","\n","param_grid = [ {\n","                'clf':[RandomForestRegressor()],\n","                'clf__n_estimators': [50,100,150],\n","                'clf__max_depth': [10,20,30]\n","                },\n","               \n","                {\n","                'clf': [ExtraTreesRegressor()],\n","                'clf__n_estimators': [50,100,150],\n","                'clf__max_depth': [10,20,30]\n","                },\n","               \n","                {\n","                'clf': [XGBRegressor()],  \n","                'clf__learning_rate':[0.1,0.05],\n","                'clf__min_samples_split':[5,7,9],\n","                'clf__max_depth':[10,20,30]\n","                }\n","              ]\n","\n","rscv_relevant_tree = RandomizedSearchCV(pipe, param_grid, cv = 3, scoring = my_score, n_jobs=-1)\n","model_relevant_tree = rscv_relevant_tree.fit(x_train_relevant, y_train_relevant)"],"metadata":{"id":"FNOHOfUmAl1R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rscv_relevant_tree.best_estimator_"],"metadata":{"id":"eqZjxMOcArhw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred= rscv_relevant_tree.best_estimator_.predict(x_test_relevant)\n","print('WMAE:', WMAE(y_test_relevant, y_pred))"],"metadata":{"id":"hxnCIDwPAsTT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5.3. HYPERPARAMETERS TUNING"],"metadata":{"id":"qc81ohZ3A0TD"}},{"cell_type":"markdown","source":["Since the model that obtained the best result was the Random Forest algorithm trained with the most relevant features, we will tune some hyperparameters to try to obtain even better results."],"metadata":{"id":"kYjnT-DrA4my"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.pipeline import Pipeline\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import make_scorer\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestRegressor\n","clf = RandomForestRegressor(random_state=0)\n","\n","pipe = Pipeline(steps=[('clf', clf)])\n","\n","param_grid_rf = [ {\n","                'clf':[RandomForestRegressor()],\n","                'clf__n_estimators': [140,150,160],\n","                'clf__max_depth': [25,30,35],\n","                'clf__max_features': ['auto',5,6]\n","                }\n","              ]\n","\n","gscv_rf1 = GridSearchCV(pipe, param_grid_rf, cv = 3, scoring = my_score, n_jobs=-1)\n","model_rf1 = gscv_rf1.fit(x_train_relevant, y_train_relevant)"],"metadata":{"id":"b6_LbRUaA5Nt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gscv_rf1.best_estimator_"],"metadata":{"id":"eEoqYENABCA4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred_rf = gscv_rf1.best_estimator_.predict(x_test_relevant)\n","print('WMAE:', WMAE(y_test_relevant, y_pred_rf))\n","WMAE: 1469.78"],"metadata":{"id":"ctxIm8HMBGKA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The \"Dept\" and \"Size\" seem to be the most important features of the model training. Although \"IsHoliday\" is used to weight the evaluation metric, a large majority of records are holiday-free, and therefore the small proportion of holiday sales records was not as deterministic for forecasting weekly sales."],"metadata":{"id":"sXNAC2otBNLk"}},{"cell_type":"code","source":["plt.rcParams[\"figure.figsize\"] = (5,3)\n","\n","importances = gscv_rf1.best_estimator_._final_estimator.feature_importances_\n","\n","attributes = list(x_train_relevant.columns)\n","indices = np.argsort(importances)\n","attributes_rank = []\n","for i in indices:\n","    attributes_rank.append(attributes[i])\n","plt.title('Feature Importances')\n","plt.tight_layout()\n","plt.barh(range(len(indices)), importances[indices], color='gray', align='center')\n","plt.yticks(range(len(indices)), attributes_rank, fontsize=5)\n","plt.xlabel('Relative Importance',fontsize=5)\n","plt.xticks(color='k', size=15)\n","plt.yticks(color='k', size=15)\n","plt.xlim([0.0, 0.25])\n","plt.show()"],"metadata":{"id":"DfVQqe8jBSpX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6. SUBMISSION PREDICTIONS"],"metadata":{"id":"Tctee9tpBf9p"}},{"cell_type":"markdown","source":["Finally, we use the training model with the lowest WMAE score to predict the test dataset values."],"metadata":{"id":"_3MYhEdpBjxP"}},{"cell_type":"code","source":["date = test['Date']\n","test = test.drop(['Date'], axis=1)"],"metadata":{"id":"zwveDqEuBkhX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_relevant = test.drop(['Temperature','Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5','CPI', 'Unemployment'],axis=1)\n","test_relevant = test_relevant.sort_values(['Store', 'Dept'], ascending=[True, True])\n","y_pred_rf = gscv_rf1.best_estimator_.predict(test_relevant)"],"metadata":{"id":"H8VOJNuRBoi7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_relevant['Date'] = date\n","test_relevant = test_relevant.sort_values(['Store', 'Dept'], ascending=[True, True])\n","test_relevant['Weekly_Sales'] = y_pred_rf\n","test_relevant"],"metadata":{"id":"lQ9pUvKeBs6g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["By plotting the weekly sales average of the training base and the predictions for the test dataset, it is possible to conclude that the forecasts appear to be consistent, able to get data pattern and sazonal component."],"metadata":{"id":"E9QfoGbOB1yH"}},{"cell_type":"code","source":["test = test_relevant\n","\n","weekly_sales_train = train.groupby(['Year','Week']).agg({'Weekly_Sales': ['mean']}).reset_index()\n","weekly_sales_test = test.groupby(['Year','Week']).agg({'Weekly_Sales': ['mean']}).reset_index()\n","\n","indices = weekly_sales_train.shape[0] + weekly_sales_test['Weekly_Sales'].index \n","plt.figure(figsize=(20, 7))\n","sns.lineplot(weekly_sales_train['Weekly_Sales'].index,weekly_sales_train['Weekly_Sales']['mean'], color='gray')\n","sns.lineplot(indices,weekly_sales_test['Weekly_Sales']['mean'],color = 'red')"],"metadata":{"id":"l8STM7VfB8jc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(20, 7))\n","\n","weekly_sales2010 = train.loc[train['Year']==2010].groupby(['Week']).agg({'Weekly_Sales': ['mean']})\n","weekly_sales2011 = train.loc[train['Year']==2011].groupby(['Week']).agg({'Weekly_Sales': ['mean']})\n","weekly_sales2012 = train.loc[train['Year']==2012].groupby(['Week']).agg({'Weekly_Sales': ['mean']})\n","weekly_sales2012_test = test.loc[test['Year']==2012].groupby(['Week']).agg({'Weekly_Sales': ['mean']})\n","weekly_sales2013_test = test.loc[test['Year']==2013].groupby(['Week']).agg({'Weekly_Sales': ['mean']})\n","\n","sns.lineplot(weekly_sales2010['Weekly_Sales']['mean'].index, weekly_sales2010['Weekly_Sales']['mean'].values, color='gray')\n","sns.lineplot(weekly_sales2011['Weekly_Sales']['mean'].index, weekly_sales2011['Weekly_Sales']['mean'].values, color='gray')\n","sns.lineplot(weekly_sales2012['Weekly_Sales']['mean'].index, weekly_sales2012['Weekly_Sales']['mean'].values, color='gray')\n","sns.lineplot(weekly_sales2012_test['Weekly_Sales']['mean'].index, weekly_sales2012_test['Weekly_Sales']['mean'].values, color='red')\n","sns.lineplot(weekly_sales2013_test['Weekly_Sales']['mean'].index, weekly_sales2013_test['Weekly_Sales']['mean'].values, color='red')\n","\n","plt.grid()\n","plt.xticks(np.arange(1, 53, step=1))\n","plt.legend(['2010', '2011', '2012','2012 test', '2013 test'])\n","plt.show()\n"],"metadata":{"id":"Sx47rXJnCEW1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["7. SUBMISSION"],"metadata":{"id":"WRuR7ffiCNGd"}},{"cell_type":"markdown","source":["After predction, we prepare the file with the results to submit it to Kaggle evaluation and check the final score."],"metadata":{"id":"VSy6FDjeCSZX"}},{"cell_type":"markdown","source":[],"metadata":{"id":"BHdz5ESyCS3J"}},{"cell_type":"code","source":["sampleSubmission = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/sampleSubmission.csv.zip', sep=',')"],"metadata":{"id":"Kn_ev_pTCTLC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sampleSubmission['Weekly_Sales'] = y_pred_rf\n","sampleSubmission.to_csv('submission.csv',index=False)\n","sampleSubmission"],"metadata":{"id":"Jk-eNXuFCYqW"},"execution_count":null,"outputs":[]}]}